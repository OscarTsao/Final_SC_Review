{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# S-C Evidence Pipeline - Stage 1: Retrieval\n",
    "\n",
    "This notebook runs the NV-Embed-v2 retrieval stage and saves candidates for reranking.\n",
    "\n",
    "**Run this notebook in the NV-Embed-v2 conda environment.**\n",
    "\n",
    "## Output\n",
    "Saves retrieval candidates to `outputs/retrieval_candidates/` for the reranker notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"..\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from final_sc_review.data.io import load_criteria, load_groundtruth, load_sentence_corpus\n",
    "from final_sc_review.data.splits import split_post_ids\n",
    "from final_sc_review.retriever.zoo import RetrieverZoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline config\n",
    "RETRIEVER_NAME = \"nv-embed-v2\"\n",
    "TOP_K_RETRIEVER = 20  # Candidates from retriever\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "GROUNDTRUTH_PATH = DATA_DIR / \"groundtruth/evidence_sentence_groundtruth.csv\"\n",
    "CORPUS_PATH = DATA_DIR / \"groundtruth/sentence_corpus.jsonl\"\n",
    "CRITERIA_PATH = DATA_DIR / \"DSM5/MDD_Criteira.json\"\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "\n",
    "# Output directory for retrieval candidates\n",
    "OUTPUT_DIR = project_root / \"outputs\" / \"retrieval_candidates\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Retriever: {RETRIEVER_NAME}\")\n",
    "print(f\"Top-K Retriever: {TOP_K_RETRIEVER}\")\n",
    "print(f\"N Folds: {N_FOLDS}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load groundtruth, sentences, and criteria\n",
    "groundtruth = load_groundtruth(GROUNDTRUTH_PATH)\n",
    "sentences = load_sentence_corpus(CORPUS_PATH)\n",
    "criteria = load_criteria(CRITERIA_PATH)\n",
    "\n",
    "print(f\"Groundtruth rows: {len(groundtruth)}\")\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "print(f\"Criteria: {len(criteria)}\")\n",
    "\n",
    "# Build lookup maps\n",
    "sentences_by_post = defaultdict(list)\n",
    "for sent in sentences:\n",
    "    sentences_by_post[sent.post_id].append(sent)\n",
    "\n",
    "sentence_map = {s.sent_uid: s for s in sentences}\n",
    "criteria_map = {c.criterion_id: c.text for c in criteria}\n",
    "\n",
    "# Get all post IDs\n",
    "all_post_ids = sorted(set(row.post_id for row in groundtruth))\n",
    "print(f\"Unique posts: {len(all_post_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Create 5-Fold Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_splits(post_ids: List[str], n_folds: int = 5, seed: int = 42) -> List[Tuple[List[str], List[str]]]:\n",
    "    \"\"\"Create k-fold cross-validation splits at post level (post-disjoint).\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    post_ids = np.array(post_ids)\n",
    "    np.random.shuffle(post_ids)\n",
    "    \n",
    "    fold_size = len(post_ids) // n_folds\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        if i == n_folds - 1:\n",
    "            end = len(post_ids)\n",
    "        else:\n",
    "            end = start + fold_size\n",
    "        \n",
    "        val_posts = post_ids[start:end].tolist()\n",
    "        train_posts = np.concatenate([post_ids[:start], post_ids[end:]]).tolist()\n",
    "        folds.append((train_posts, val_posts))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Create folds\n",
    "folds = create_kfold_splits(all_post_ids, n_folds=N_FOLDS)\n",
    "\n",
    "for i, (train_posts, val_posts) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}: Train={len(train_posts)} posts, Val={len(val_posts)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Build Query Data from Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_data(groundtruth, post_ids: Set[str], include_no_evidence: bool = True):\n",
    "    \"\"\"Build query data from groundtruth for given post IDs.\n",
    "    \n",
    "    Returns list of dicts with:\n",
    "        - post_id, criterion_id, criterion_text\n",
    "        - gold_uids: set of positive sentence UIDs\n",
    "        - is_no_evidence: whether this query has no positives\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    \n",
    "    for row in groundtruth:\n",
    "        if row.post_id not in post_ids:\n",
    "            continue\n",
    "        \n",
    "        key = (row.post_id, row.criterion_id)\n",
    "        if key not in queries:\n",
    "            queries[key] = {\n",
    "                'post_id': row.post_id,\n",
    "                'criterion_id': row.criterion_id,\n",
    "                'criterion_text': criteria_map.get(row.criterion_id, row.criterion_id),\n",
    "                'gold_uids': set(),\n",
    "            }\n",
    "        \n",
    "        if row.groundtruth == 1:\n",
    "            queries[key]['gold_uids'].add(row.sent_uid)\n",
    "    \n",
    "    result = []\n",
    "    for query_data in queries.values():\n",
    "        query_data['is_no_evidence'] = len(query_data['gold_uids']) == 0\n",
    "        if include_no_evidence or not query_data['is_no_evidence']:\n",
    "            result.append(query_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with all posts\n",
    "all_queries = build_query_data(groundtruth, set(all_post_ids), include_no_evidence=True)\n",
    "queries_with_evidence = [q for q in all_queries if not q['is_no_evidence']]\n",
    "queries_no_evidence = [q for q in all_queries if q['is_no_evidence']]\n",
    "\n",
    "print(f\"Total queries: {len(all_queries)}\")\n",
    "print(f\"Queries with evidence: {len(queries_with_evidence)}\")\n",
    "print(f\"Queries with no evidence: {len(queries_no_evidence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever (shared across folds)\n",
    "retriever_zoo = RetrieverZoo(sentences=sentences, cache_dir=CACHE_DIR)\n",
    "retriever = retriever_zoo.get_retriever(RETRIEVER_NAME)\n",
    "\n",
    "# Encode corpus (uses cache if available)\n",
    "print(f\"Encoding corpus for {RETRIEVER_NAME}...\")\n",
    "retriever.encode_corpus(rebuild=False)\n",
    "print(\"Retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Prepare Retrieval Data for Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_retrieval_data(queries, retriever, top_k: int = 20):\n",
    "    \"\"\"Prepare retrieval data by retrieving candidates for each query.\"\"\"\n",
    "    retrieval_data = []\n",
    "    \n",
    "    for query in tqdm(queries, desc=\"Retrieving candidates\"):\n",
    "        post_id = query['post_id']\n",
    "        criterion_text = query['criterion_text']\n",
    "        gold_uids = query['gold_uids']\n",
    "        \n",
    "        # Get sentences for this post\n",
    "        post_sentences = sentences_by_post.get(post_id, [])\n",
    "        if len(post_sentences) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Retrieve candidates\n",
    "        try:\n",
    "            results = retriever.retrieve_within_post(\n",
    "                query=criterion_text,\n",
    "                post_id=post_id,\n",
    "                top_k=min(top_k, len(post_sentences)),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving for {post_id}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if len(results) < 2:\n",
    "            continue\n",
    "        \n",
    "        candidates = []\n",
    "        for r in results:\n",
    "            candidates.append({\n",
    "                'sent_uid': r.sent_uid,\n",
    "                'text': r.text,\n",
    "                'score': float(r.score),\n",
    "                'label': 1 if r.sent_uid in gold_uids else 0,\n",
    "            })\n",
    "        \n",
    "        retrieval_data.append({\n",
    "            'query': criterion_text,\n",
    "            'post_id': post_id,\n",
    "            'criterion_id': query['criterion_id'],\n",
    "            'gold_uids': list(gold_uids),\n",
    "            'is_no_evidence': query['is_no_evidence'],\n",
    "            'candidates': candidates,\n",
    "        })\n",
    "    \n",
    "    return retrieval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run retrieval for each fold and save results\n",
    "all_fold_data = {}\n",
    "\n",
    "for fold_idx, (train_posts, val_posts) in enumerate(folds):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train posts: {len(train_posts)}, Val posts: {len(val_posts)}\")\n",
    "    \n",
    "    # Build query data for this fold\n",
    "    train_queries = build_query_data(groundtruth, set(train_posts), include_no_evidence=True)\n",
    "    val_queries = build_query_data(groundtruth, set(val_posts), include_no_evidence=True)\n",
    "    \n",
    "    print(f\"Train queries: {len(train_queries)}, Val queries: {len(val_queries)}\")\n",
    "    \n",
    "    # Prepare training and validation data with retriever candidates\n",
    "    print(\"\\nPreparing training data...\")\n",
    "    train_data = prepare_retrieval_data(train_queries, retriever, top_k=TOP_K_RETRIEVER)\n",
    "    \n",
    "    print(\"Preparing validation data...\")\n",
    "    val_data = prepare_retrieval_data(val_queries, retriever, top_k=TOP_K_RETRIEVER)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    # Store fold data\n",
    "    all_fold_data[f'fold_{fold_idx + 1}'] = {\n",
    "        'train_posts': train_posts,\n",
    "        'val_posts': val_posts,\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data,\n",
    "    }\n",
    "\n",
    "print(f\"\\nRetrieval complete for all {N_FOLDS} folds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Save Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration metadata\n",
    "config_metadata = {\n",
    "    'retriever_name': RETRIEVER_NAME,\n",
    "    'top_k_retriever': TOP_K_RETRIEVER,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'total_posts': len(all_post_ids),\n",
    "    'total_queries': len(all_queries),\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"config_metadata.json\", \"w\") as f:\n",
    "    json.dump(config_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Saved config metadata to {OUTPUT_DIR / 'config_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fold data using pickle (efficient for large internal data)\n",
    "# Note: This is internal pipeline data, not untrusted external content\n",
    "output_file = OUTPUT_DIR / \"retrieval_candidates.pkl\"\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(all_fold_data, f)\n",
    "\n",
    "# Get file size\n",
    "file_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "print(f\"Saved retrieval candidates to {output_file}\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save as JSON for inspection (optional, may be large)\n",
    "json_output_file = OUTPUT_DIR / \"retrieval_candidates.json\"\n",
    "\n",
    "with open(json_output_file, \"w\") as f:\n",
    "    json.dump(all_fold_data, f, indent=2)\n",
    "\n",
    "json_file_size_mb = json_output_file.stat().st_size / (1024 * 1024)\n",
    "print(f\"Saved JSON backup to {json_output_file}\")\n",
    "print(f\"File size: {json_file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL STAGE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Retriever: {RETRIEVER_NAME}\")\n",
    "print(f\"  - Top-K: {TOP_K_RETRIEVER}\")\n",
    "print(f\"  - N Folds: {N_FOLDS}\")\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "for fold_name, fold_data in all_fold_data.items():\n",
    "    print(f\"  - {fold_name}: {len(fold_data['train_data'])} train, {len(fold_data['val_data'])} val samples\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - {OUTPUT_DIR / 'config_metadata.json'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'retrieval_candidates.pkl'} ({file_size_mb:.2f} MB)\")\n",
    "print(f\"  - {OUTPUT_DIR / 'retrieval_candidates.json'} ({json_file_size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nNext Step:\")\n",
    "print(f\"  Run sc_reranker_pipeline_no_postProcessing.ipynb in the Jina reranker environment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
