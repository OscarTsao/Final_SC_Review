{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S-C Evidence Pipeline Evaluation (No Post-processing)\n",
    "\n",
    "This notebook implements the NV-Embed-v2 + jina-reranker-v3 pipeline with the best HPO config (Trial 33) and runs 5-fold cross-validation with comprehensive metrics.\n",
    "\n",
    "## Best Config (Trial 33, nDCG@10=0.8205)\n",
    "- **Retriever:** NV-Embed-v2\n",
    "- **Reranker:** jina-reranker-v3 (LoRA fine-tuned)\n",
    "- **Training:** no_evidence=True (includes queries with no positive evidence)\n",
    "- **Loss:** BCE + Pairwise-Softplus + Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"..\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from final_sc_review.data.io import load_criteria, load_groundtruth, load_sentence_corpus\n",
    "from final_sc_review.data.splits import split_post_ids\n",
    "from final_sc_review.metrics.ranking import ndcg_at_k, recall_at_k, mrr_at_k, map_at_k\n",
    "from final_sc_review.retriever.zoo import RetrieverZoo\n",
    "from final_sc_review.reranker.losses import HybridRerankerLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best Trial 33 hyperparameters\nBEST_PARAMS = {\n    'batch_size': 1,\n    'num_epochs': 1,\n    'learning_rate': 4.447467238603695e-05,\n    'weight_decay': 8.769982161626777e-05,\n    'grad_accum': 2,\n    'pointwise_type': 'bce',\n    'pairwise_type': 'pairwise_softplus',\n    'listwise_type': 'lambda',\n    'w_list': 1.0755666826190335,\n    'w_pair': 1.8398728897689836,\n    'w_point': 0.813832693617893,\n    'temperature': 0.9342605824607415,\n    'sigma': 1.5735217400312576,\n    'margin': 0.7247599691970003,\n    'max_pairs': 100,\n    'lora_r': 16,\n    'lora_alpha': 16,\n    'lora_dropout': 0.05,\n}\n\n# Pipeline config\nRETRIEVER_NAME = \"nv-embed-v2\"\n# Use BGE-reranker-v2-m3 (compatible with transformers 4.44.2)\n# Note: jina-reranker-v3 requires newer transformers which breaks NV-Embed-v2\nRERANKER_MODEL_ID = \"BAAI/bge-reranker-v2-m3\"\nTOP_K_RETRIEVER = 20  # Candidates from retriever\nN_FOLDS = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Data paths\nDATA_DIR = project_root / \"data\"\nGROUNDTRUTH_PATH = DATA_DIR / \"groundtruth/evidence_sentence_groundtruth.csv\"\nCORPUS_PATH = DATA_DIR / \"groundtruth/sentence_corpus.jsonl\"\nCRITERIA_PATH = DATA_DIR / \"DSM5/MDD_Criteira.json\"\n# Note: RetrieverZoo internally adds '/retriever_zoo' subdirectory\nCACHE_DIR = DATA_DIR / \"cache\"\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Retriever: {RETRIEVER_NAME}\")\nprint(f\"Reranker: {RERANKER_MODEL_ID}\")\nprint(f\"Top-K Retriever: {TOP_K_RETRIEVER}\")\nprint(f\"N Folds: {N_FOLDS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load groundtruth, sentences, and criteria\n",
    "groundtruth = load_groundtruth(GROUNDTRUTH_PATH)\n",
    "sentences = load_sentence_corpus(CORPUS_PATH)\n",
    "criteria = load_criteria(CRITERIA_PATH)\n",
    "\n",
    "print(f\"Groundtruth rows: {len(groundtruth)}\")\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "print(f\"Criteria: {len(criteria)}\")\n",
    "\n",
    "# Build lookup maps\n",
    "sentences_by_post = defaultdict(list)\n",
    "for sent in sentences:\n",
    "    sentences_by_post[sent.post_id].append(sent)\n",
    "\n",
    "sentence_map = {s.sent_uid: s for s in sentences}\n",
    "criteria_map = {c.criterion_id: c.text for c in criteria}\n",
    "\n",
    "# Get all post IDs\n",
    "all_post_ids = sorted(set(row.post_id for row in groundtruth))\n",
    "print(f\"Unique posts: {len(all_post_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create 5-Fold Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_splits(post_ids: List[str], n_folds: int = 5, seed: int = 42) -> List[Tuple[List[str], List[str]]]:\n",
    "    \"\"\"Create k-fold cross-validation splits at post level (post-disjoint).\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    post_ids = np.array(post_ids)\n",
    "    np.random.shuffle(post_ids)\n",
    "    \n",
    "    fold_size = len(post_ids) // n_folds\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        if i == n_folds - 1:\n",
    "            end = len(post_ids)\n",
    "        else:\n",
    "            end = start + fold_size\n",
    "        \n",
    "        val_posts = post_ids[start:end].tolist()\n",
    "        train_posts = np.concatenate([post_ids[:start], post_ids[end:]]).tolist()\n",
    "        folds.append((train_posts, val_posts))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Create folds\n",
    "folds = create_kfold_splits(all_post_ids, n_folds=N_FOLDS)\n",
    "\n",
    "for i, (train_posts, val_posts) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}: Train={len(train_posts)} posts, Val={len(val_posts)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Query Data from Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_data(groundtruth, post_ids: Set[str], include_no_evidence: bool = True):\n",
    "    \"\"\"Build query data from groundtruth for given post IDs.\n",
    "    \n",
    "    Returns list of dicts with:\n",
    "        - post_id, criterion_id, criterion_text\n",
    "        - gold_uids: set of positive sentence UIDs\n",
    "        - is_no_evidence: whether this query has no positives\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    \n",
    "    for row in groundtruth:\n",
    "        if row.post_id not in post_ids:\n",
    "            continue\n",
    "        \n",
    "        key = (row.post_id, row.criterion_id)\n",
    "        if key not in queries:\n",
    "            queries[key] = {\n",
    "                'post_id': row.post_id,\n",
    "                'criterion_id': row.criterion_id,\n",
    "                'criterion_text': criteria_map.get(row.criterion_id, row.criterion_id),\n",
    "                'gold_uids': set(),\n",
    "            }\n",
    "        \n",
    "        if row.groundtruth == 1:\n",
    "            queries[key]['gold_uids'].add(row.sent_uid)\n",
    "    \n",
    "    result = []\n",
    "    for query_data in queries.values():\n",
    "        query_data['is_no_evidence'] = len(query_data['gold_uids']) == 0\n",
    "        if include_no_evidence or not query_data['is_no_evidence']:\n",
    "            result.append(query_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with all posts\n",
    "all_queries = build_query_data(groundtruth, set(all_post_ids), include_no_evidence=True)\n",
    "queries_with_evidence = [q for q in all_queries if not q['is_no_evidence']]\n",
    "queries_no_evidence = [q for q in all_queries if q['is_no_evidence']]\n",
    "\n",
    "print(f\"Total queries: {len(all_queries)}\")\n",
    "print(f\"Queries with evidence: {len(queries_with_evidence)}\")\n",
    "print(f\"Queries with no evidence: {len(queries_no_evidence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever (shared across folds)\n",
    "retriever_zoo = RetrieverZoo(sentences=sentences, cache_dir=CACHE_DIR)\n",
    "retriever = retriever_zoo.get_retriever(RETRIEVER_NAME)\n",
    "\n",
    "# Encode corpus (uses cache if available)\n",
    "print(f\"Encoding corpus for {RETRIEVER_NAME}...\")\n",
    "retriever.encode_corpus(rebuild=False)\n",
    "print(\"Retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Training and Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_training_data(queries, retriever, top_k: int = 20):\n    \"\"\"Prepare training data by retrieving candidates for each query.\"\"\"\n    train_data = []\n    \n    for query in tqdm(queries, desc=\"Preparing training data\"):\n        post_id = query['post_id']\n        criterion_text = query['criterion_text']\n        gold_uids = query['gold_uids']\n        \n        # Get sentences for this post\n        post_sentences = sentences_by_post.get(post_id, [])\n        if len(post_sentences) < 2:\n            continue\n        \n        # Retrieve candidates\n        try:\n            results = retriever.retrieve_within_post(\n                query=criterion_text,\n                post_id=post_id,\n                top_k=min(top_k, len(post_sentences)),\n            )\n        except Exception as e:\n            continue\n        \n        if len(results) < 2:\n            continue\n        \n        candidates = []\n        for r in results:\n            candidates.append({\n                'sent_uid': r.sent_uid,\n                'text': r.text,\n                'score': r.score,\n                'label': 1 if r.sent_uid in gold_uids else 0,\n            })\n        \n        train_data.append({\n            'query': criterion_text,\n            'post_id': post_id,\n            'criterion_id': query['criterion_id'],\n            'gold_uids': list(gold_uids),\n            'is_no_evidence': query['is_no_evidence'],\n            'candidates': candidates,\n        })\n    \n    return train_data\n\n\ndef train_reranker(train_data, params, verbose=True):\n    \"\"\"Train a LoRA-adapted reranker on the training data.\"\"\"\n    # Load fresh model\n    tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_ID, trust_remote_code=True)\n    # Set padding token if not defined (required for batch processing)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load model - use float16 for BGE-reranker-v2-m3 (doesn't support bfloat16)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        RERANKER_MODEL_ID,\n        torch_dtype=torch.float16,\n        trust_remote_code=True\n    )\n    # Set pad_token_id on model config\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    # Apply LoRA - target modules differ by model architecture\n    # BGE-reranker-v2-m3 (RoBERTa): query, key, value\n    # jina-reranker-v3 (Qwen3): q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n    if \"bge\" in RERANKER_MODEL_ID.lower():\n        target_modules = [\"query\", \"key\", \"value\"]\n    else:\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    \n    lora_config = LoraConfig(\n        r=params['lora_r'],\n        lora_alpha=params['lora_alpha'],\n        lora_dropout=params['lora_dropout'],\n        target_modules=target_modules,\n        bias=\"none\",\n        task_type=\"SEQ_CLS\",\n    )\n    model = get_peft_model(model, lora_config)\n    model = model.to(DEVICE)\n    \n    # Create loss function\n    loss_fn = HybridRerankerLoss(\n        pointwise_type=params['pointwise_type'],\n        pairwise_type=params['pairwise_type'],\n        listwise_type=params['listwise_type'],\n        w_point=params['w_point'],\n        w_pair=params['w_pair'],\n        w_list=params['w_list'],\n        temperature=params['temperature'],\n        sigma=params['sigma'],\n        margin=params['margin'],\n    )\n    \n    # Optimizer\n    optimizer = AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n    \n    # Training\n    model.train()\n    total_loss = 0\n    step = 0\n    \n    # Shuffle training data\n    np.random.shuffle(train_data)\n    \n    for epoch in range(params['num_epochs']):\n        for query_data in train_data:\n            query_text = query_data['query']\n            candidates = query_data['candidates'][:TOP_K_RETRIEVER]\n            \n            if len(candidates) < 2:\n                continue\n            \n            # Prepare inputs\n            texts = [[query_text, c['text']] for c in candidates]\n            labels = torch.tensor([c['label'] for c in candidates], dtype=torch.float32, device=DEVICE)\n            \n            inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            \n            # Forward\n            outputs = model(**inputs)\n            scores = outputs.logits.squeeze(-1)\n            \n            # Loss\n            loss = loss_fn(scores, labels)\n            loss = loss / params['grad_accum']\n            loss.backward()\n            \n            step += 1\n            if step % params['grad_accum'] == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            total_loss += loss.item() * params['grad_accum']\n    \n    avg_loss = total_loss / step if step > 0 else 0\n    if verbose:\n        print(f\"  Training done. Steps: {step}, Avg loss: {avg_loss:.4f}\")\n    \n    return model, tokenizer\n\n\ndef assess_reranker(model, tokenizer, val_data, verbose=True):\n    \"\"\"Assess reranker on validation data.\n    \n    Returns:\n        - metrics: dict with aggregated metrics\n        - detailed_results: list of per-query results for investigation\n    \"\"\"\n    model.eval()\n    \n    all_results = {k: [] for k in [1, 5, 10, 20]}\n    detailed_results = []\n    \n    with torch.no_grad():\n        for query_data in tqdm(val_data, desc=\"Assessing\", disable=not verbose):\n            query_text = query_data['query']\n            candidates = query_data['candidates'][:TOP_K_RETRIEVER]\n            gold_uids = set(query_data['gold_uids'])\n            \n            if len(candidates) < 2:\n                continue\n            \n            # Skip queries with no positives for metric calculation\n            if not gold_uids:\n                continue\n            \n            # Score candidates\n            texts = [[query_text, c['text']] for c in candidates]\n            inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            \n            outputs = model(**inputs)\n            scores = outputs.logits.squeeze(-1).cpu().numpy()\n            \n            # Get retriever-only ranking\n            retriever_ranking = [c['sent_uid'] for c in candidates]\n            retriever_scores = [c['score'] for c in candidates]\n            \n            # Rerank by score\n            ranked_indices = np.argsort(scores)[::-1]\n            reranked_candidates = [candidates[i] for i in ranked_indices]\n            reranked_ids = [candidates[i]['sent_uid'] for i in ranked_indices]\n            reranked_scores = [float(scores[i]) for i in ranked_indices]\n            \n            # Compute metrics for each k\n            query_metrics = {}\n            for k in [1, 5, 10, 20]:\n                # Retriever metrics\n                ret_ndcg = ndcg_at_k(gold_uids, retriever_ranking, k)\n                ret_recall = recall_at_k(gold_uids, retriever_ranking, k)\n                ret_mrr = mrr_at_k(gold_uids, retriever_ranking, k)\n                ret_map = map_at_k(gold_uids, retriever_ranking, k)\n                \n                # Reranker metrics\n                rerank_ndcg = ndcg_at_k(gold_uids, reranked_ids, k)\n                rerank_recall = recall_at_k(gold_uids, reranked_ids, k)\n                rerank_mrr = mrr_at_k(gold_uids, reranked_ids, k)\n                rerank_map = map_at_k(gold_uids, reranked_ids, k)\n                \n                all_results[k].append({\n                    'ret_ndcg': ret_ndcg, 'ret_recall': ret_recall, 'ret_mrr': ret_mrr, 'ret_map': ret_map,\n                    'rerank_ndcg': rerank_ndcg, 'rerank_recall': rerank_recall, \n                    'rerank_mrr': rerank_mrr, 'rerank_map': rerank_map,\n                })\n                \n                query_metrics[f'ndcg@{k}'] = {'retriever': ret_ndcg, 'reranker': rerank_ndcg}\n                query_metrics[f'recall@{k}'] = {'retriever': ret_recall, 'reranker': rerank_recall}\n            \n            # Store detailed result for investigation\n            detailed_results.append({\n                'post_id': query_data['post_id'],\n                'criterion_id': query_data['criterion_id'],\n                'query_text': query_text,\n                'gold_uids': list(gold_uids),\n                'retriever_ranking': [\n                    {'sent_uid': c['sent_uid'], 'text': c['text'][:100], 'score': c['score'], \n                     'is_positive': c['sent_uid'] in gold_uids}\n                    for c in candidates\n                ],\n                'reranker_ranking': [\n                    {'sent_uid': reranked_candidates[i]['sent_uid'], \n                     'text': reranked_candidates[i]['text'][:100],\n                     'reranker_score': reranked_scores[i],\n                     'is_positive': reranked_candidates[i]['sent_uid'] in gold_uids}\n                    for i in range(len(reranked_candidates))\n                ],\n                'metrics': query_metrics,\n            })\n    \n    # Aggregate metrics\n    metrics = {}\n    for k in [1, 5, 10, 20]:\n        if all_results[k]:\n            metrics[f'ret_ndcg@{k}'] = np.mean([r['ret_ndcg'] for r in all_results[k]])\n            metrics[f'ret_recall@{k}'] = np.mean([r['ret_recall'] for r in all_results[k]])\n            metrics[f'ret_mrr@{k}'] = np.mean([r['ret_mrr'] for r in all_results[k]])\n            metrics[f'ret_map@{k}'] = np.mean([r['ret_map'] for r in all_results[k]])\n            \n            metrics[f'rerank_ndcg@{k}'] = np.mean([r['rerank_ndcg'] for r in all_results[k]])\n            metrics[f'rerank_recall@{k}'] = np.mean([r['rerank_recall'] for r in all_results[k]])\n            metrics[f'rerank_mrr@{k}'] = np.mean([r['rerank_mrr'] for r in all_results[k]])\n            metrics[f'rerank_map@{k}'] = np.mean([r['rerank_map'] for r in all_results[k]])\n    \n    metrics['n_queries'] = len(all_results[10])\n    \n    return metrics, detailed_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "fold_metrics = []\n",
    "all_detailed_results = []\n",
    "\n",
    "for fold_idx, (train_posts, val_posts) in enumerate(folds):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train posts: {len(train_posts)}, Val posts: {len(val_posts)}\")\n",
    "    \n",
    "    # Build query data for this fold\n",
    "    train_queries = build_query_data(groundtruth, set(train_posts), include_no_evidence=True)\n",
    "    val_queries = build_query_data(groundtruth, set(val_posts), include_no_evidence=True)\n",
    "    \n",
    "    print(f\"Train queries: {len(train_queries)}, Val queries: {len(val_queries)}\")\n",
    "    \n",
    "    # Prepare training and validation data with retriever candidates\n",
    "    print(\"\\nPreparing training data...\")\n",
    "    train_data = prepare_training_data(train_queries, retriever, top_k=TOP_K_RETRIEVER)\n",
    "    \n",
    "    print(\"Preparing validation data...\")\n",
    "    val_data = prepare_training_data(val_queries, retriever, top_k=TOP_K_RETRIEVER)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    # Train reranker\n",
    "    print(\"\\nTraining reranker...\")\n",
    "    model, tokenizer = train_reranker(train_data, BEST_PARAMS, verbose=True)\n",
    "    \n",
    "    # Assess\n",
    "    print(\"\\nAssessing...\")\n",
    "    metrics, detailed = assess_reranker(model, tokenizer, val_data, verbose=True)\n",
    "    \n",
    "    # Add fold info to detailed results\n",
    "    for d in detailed:\n",
    "        d['fold'] = fold_idx + 1\n",
    "    \n",
    "    fold_metrics.append(metrics)\n",
    "    all_detailed_results.extend(detailed)\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f\"\\nFold {fold_idx + 1} Results:\")\n",
    "    print(f\"  Queries: {metrics['n_queries']}\")\n",
    "    print(f\"  Retriever nDCG@10: {metrics['ret_ndcg@10']:.4f}\")\n",
    "    print(f\"  Reranker nDCG@10:  {metrics['rerank_ndcg@10']:.4f} (+{metrics['rerank_ndcg@10'] - metrics['ret_ndcg@10']:.4f})\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggregate Results Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute mean and std across folds\naggregated = {}\nfor key in fold_metrics[0].keys():\n    if key == 'n_queries':\n        aggregated[key] = sum(m[key] for m in fold_metrics)\n    else:\n        values = [m[key] for m in fold_metrics]\n        aggregated[f'{key}_mean'] = np.mean(values)\n        aggregated[f'{key}_std'] = np.std(values)\n\n# Print comprehensive results table\nprint(\"\\n\" + \"=\"*80)\nprint(\"5-FOLD CROSS-VALIDATION RESULTS\")\nprint(f\"Model: NV-Embed-v2 + {RERANKER_MODEL_ID.split('/')[-1]} (Trial 33 config, no_evidence=True)\")\nprint(\"=\"*80)\n\nprint(f\"\\nTotal queries assessed: {aggregated['n_queries']}\")\n\nprint(f\"\\n{'Metric':<15} {'@1':>12} {'@5':>12} {'@10':>12} {'@20':>12}\")\nprint(\"-\"*65)\n\n# Retriever metrics\nfor metric_name in ['ndcg', 'recall', 'mrr', 'map']:\n    row = f\"Ret {metric_name.upper():<10}\"\n    for k in [1, 5, 10, 20]:\n        mean = aggregated[f'ret_{metric_name}@{k}_mean']\n        std = aggregated[f'ret_{metric_name}@{k}_std']\n        row += f\" {mean:.4f}+/-{std:.3f}\"\n    print(row)\n\nprint(\"-\"*65)\n\n# Reranker metrics\nfor metric_name in ['ndcg', 'recall', 'mrr', 'map']:\n    row = f\"Rerank {metric_name.upper():<7}\"\n    for k in [1, 5, 10, 20]:\n        mean = aggregated[f'rerank_{metric_name}@{k}_mean']\n        std = aggregated[f'rerank_{metric_name}@{k}_std']\n        row += f\" {mean:.4f}+/-{std:.3f}\"\n    print(row)\n\nprint(\"-\"*65)\n\n# Improvement\nprint(\"\\nImprovement (Reranker - Retriever):\")\nfor metric_name in ['ndcg', 'recall', 'mrr', 'map']:\n    row = f\"{metric_name.upper():<12}\"\n    for k in [1, 5, 10, 20]:\n        ret_mean = aggregated[f'ret_{metric_name}@{k}_mean']\n        rerank_mean = aggregated[f'rerank_{metric_name}@{k}_mean']\n        diff = rerank_mean - ret_mean\n        pct = (diff / ret_mean * 100) if ret_mean > 0 else 0\n        row += f\" {diff:+.4f} ({pct:+.1f}%)\"\n    print(row)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for fold-by-fold comparison\n",
    "fold_df = pd.DataFrame(fold_metrics)\n",
    "fold_df.index = [f\"Fold {i+1}\" for i in range(len(fold_metrics))]\n",
    "\n",
    "# Show key metrics per fold\n",
    "key_metrics = ['ret_ndcg@10', 'rerank_ndcg@10', 'ret_recall@10', 'rerank_recall@10', 'n_queries']\n",
    "print(\"\\nPer-Fold Metrics:\")\n",
    "display(fold_df[key_metrics].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Results for Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "detailed_df = pd.DataFrame([\n",
    "    {\n",
    "        'fold': d['fold'],\n",
    "        'post_id': d['post_id'],\n",
    "        'criterion_id': d['criterion_id'],\n",
    "        'query_text': d['query_text'][:50] + '...',\n",
    "        'n_gold': len(d['gold_uids']),\n",
    "        'ret_ndcg@10': d['metrics']['ndcg@10']['retriever'],\n",
    "        'rerank_ndcg@10': d['metrics']['ndcg@10']['reranker'],\n",
    "        'improvement': d['metrics']['ndcg@10']['reranker'] - d['metrics']['ndcg@10']['retriever'],\n",
    "    }\n",
    "    for d in all_detailed_results\n",
    "])\n",
    "\n",
    "print(f\"Total detailed results: {len(detailed_df)}\")\n",
    "display(detailed_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show queries where reranker helped most\n",
    "print(\"\\nQueries where reranker improved most:\")\n",
    "top_improvements = detailed_df.nlargest(10, 'improvement')\n",
    "display(top_improvements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show queries where reranker hurt most\n",
    "print(\"\\nQueries where reranker hurt most:\")\n",
    "worst_regressions = detailed_df.nsmallest(10, 'improvement')\n",
    "display(worst_regressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a specific query in detail\n",
    "def show_query_detail(query_idx: int):\n",
    "    \"\"\"Show detailed retriever vs reranker comparison for a specific query.\"\"\"\n",
    "    d = all_detailed_results[query_idx]\n",
    "    \n",
    "    print(f\"Post ID: {d['post_id']}\")\n",
    "    print(f\"Criterion: {d['criterion_id']}\")\n",
    "    print(f\"Query: {d['query_text']}\")\n",
    "    print(f\"Gold UIDs: {d['gold_uids']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RETRIEVER RANKING:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, c in enumerate(d['retriever_ranking'][:10], 1):\n",
    "        marker = \"[+]\" if c['is_positive'] else \"[ ]\"\n",
    "        print(f\"{i:2d}. {marker} {c['sent_uid']} (score: {c['score']:.4f})\")\n",
    "        print(f\"     {c['text']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"RERANKER RANKING:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, c in enumerate(d['reranker_ranking'][:10], 1):\n",
    "        marker = \"[+]\" if c['is_positive'] else \"[ ]\"\n",
    "        print(f\"{i:2d}. {marker} {c['sent_uid']} (score: {c['reranker_score']:.4f})\")\n",
    "        print(f\"     {c['text']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"METRICS:\")\n",
    "    for k in [1, 5, 10]:\n",
    "        ret = d['metrics'][f'ndcg@{k}']['retriever']\n",
    "        rerank = d['metrics'][f'ndcg@{k}']['reranker']\n",
    "        print(f\"  nDCG@{k}: Retriever={ret:.4f}, Reranker={rerank:.4f}, Delta={rerank-ret:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a query where reranker helped\n",
    "best_idx = detailed_df['improvement'].idxmax()\n",
    "print(\"QUERY WITH BEST RERANKER IMPROVEMENT:\")\n",
    "print(\"=\" * 80)\n",
    "show_query_detail(best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a query where reranker hurt\n",
    "worst_idx = detailed_df['improvement'].idxmin()\n",
    "print(\"QUERY WITH WORST RERANKER REGRESSION:\")\n",
    "print(\"=\" * 80)\n",
    "show_query_detail(worst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated metrics\n",
    "output_dir = project_root / \"outputs\" / \"5fold_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save aggregated metrics\n",
    "with open(output_dir / \"aggregated_metrics.json\", \"w\") as f:\n",
    "    json.dump(aggregated, f, indent=2)\n",
    "\n",
    "# Save per-fold metrics\n",
    "fold_df.to_csv(output_dir / \"fold_metrics.csv\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_df.to_csv(output_dir / \"detailed_results.csv\", index=False)\n",
    "\n",
    "# Save full detailed results (including rankings) as JSON\n",
    "with open(output_dir / \"full_detailed_results.json\", \"w\") as f:\n",
    "    json.dump(all_detailed_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(f\"  - aggregated_metrics.json\")\n",
    "print(f\"  - fold_metrics.csv\")\n",
    "print(f\"  - detailed_results.csv\")\n",
    "print(f\"  - full_detailed_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Overall Performance:\")\n",
    "print(f\"   - Retriever nDCG@10: {aggregated['ret_ndcg@10_mean']:.4f} +/- {aggregated['ret_ndcg@10_std']:.4f}\")\n",
    "print(f\"   - Reranker nDCG@10:  {aggregated['rerank_ndcg@10_mean']:.4f} +/- {aggregated['rerank_ndcg@10_std']:.4f}\")\n",
    "improvement = aggregated['rerank_ndcg@10_mean'] - aggregated['ret_ndcg@10_mean']\n",
    "pct_improvement = improvement / aggregated['ret_ndcg@10_mean'] * 100\n",
    "print(f\"   - Improvement: +{improvement:.4f} ({pct_improvement:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. Query Analysis:\")\n",
    "improved = (detailed_df['improvement'] > 0).sum()\n",
    "unchanged = (detailed_df['improvement'] == 0).sum()\n",
    "regressed = (detailed_df['improvement'] < 0).sum()\n",
    "print(f\"   - Improved: {improved} ({improved/len(detailed_df)*100:.1f}%)\")\n",
    "print(f\"   - Unchanged: {unchanged} ({unchanged/len(detailed_df)*100:.1f}%)\")\n",
    "print(f\"   - Regressed: {regressed} ({regressed/len(detailed_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. Best Config (Trial 33):\")\n",
    "print(f\"   - Loss: BCE + Pairwise-Softplus + Lambda\")\n",
    "print(f\"   - Weights: w_point={BEST_PARAMS['w_point']:.3f}, w_pair={BEST_PARAMS['w_pair']:.3f}, w_list={BEST_PARAMS['w_list']:.3f}\")\n",
    "print(f\"   - Learning rate: {BEST_PARAMS['learning_rate']:.2e}\")\n",
    "print(f\"   - LoRA: r={BEST_PARAMS['lora_r']}, alpha={BEST_PARAMS['lora_alpha']}\")\n",
    "\n",
    "print(f\"\\n4. Next Steps:\")\n",
    "print(f\"   - Investigate queries where reranker regressed\")\n",
    "print(f\"   - Analyze error patterns by criterion type\")\n",
    "print(f\"   - Consider per-criterion fine-tuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}